{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MAY2704/ML_QEA_usecases/blob/main/ETL_spark_tests_3_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iku0cO2w2FTf",
        "outputId": "30320c5e-1086-4751-c8ae-13a5a72277e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=9fca88ef29116b6169d58f4c469b60f862ce7ff8a8a9334bf9d6d985536aac0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|  Alpha| 45|\n",
            "|  2|   Beta| 76|\n",
            "|  3|Charlie| 30|\n",
            "|  4|  Delta| 70|\n",
            "|  5|   Echo| 26|\n",
            "+---+-------+---+\n",
            "\n",
            "+---+-------------+\n",
            "| id|       street|\n",
            "+---+-------------+\n",
            "|  1|  Barbastraat|\n",
            "|  2| Michalstraat|\n",
            "|  3|  Parijstraat|\n",
            "|  4|Tiensesstraat|\n",
            "|  5|  Dieststraat|\n",
            "+---+-------------+\n",
            "\n",
            "+---+-------+---+-------------+\n",
            "| id|   name|age|       street|\n",
            "+---+-------+---+-------------+\n",
            "|  1|  Alpha| 45|  Barbastraat|\n",
            "|  2|   Beta| 76| Michalstraat|\n",
            "|  3|Charlie| 30|  Parijstraat|\n",
            "|  4|  Delta| 70|Tiensesstraat|\n",
            "|  5|   Echo| 26|  Dieststraat|\n",
            "+---+-------+---+-------------+\n",
            "\n",
            "+---+-------+---+-------------+------------+\n",
            "| id|   name|age|       street|age_category|\n",
            "+---+-------+---+-------------+------------+\n",
            "|  1|  Alpha| 45|  Barbastraat|      Medior|\n",
            "|  2|   Beta| 76| Michalstraat|      Senior|\n",
            "|  3|Charlie| 30|  Parijstraat|      Medior|\n",
            "|  4|  Delta| 70|Tiensesstraat|      Senior|\n",
            "|  5|   Echo| 26|  Dieststraat|      Medior|\n",
            "+---+-------+---+-------------+------------+\n",
            "\n",
            "+---+-----+---+------------+\n",
            "| id| name|age|age_category|\n",
            "+---+-----+---+------------+\n",
            "|  1|Test1| 35|      Medior|\n",
            "|  2|Test2| 72|      Senior|\n",
            "|  3|Test3| 16|      Junior|\n",
            "|  4|Test4| 28|      Medior|\n",
            "|  5|Test5| 60|      Senior|\n",
            "+---+-----+---+------------+\n",
            "\n",
            "+---+-----+---+------------+\n",
            "| id| name|age|age_category|\n",
            "+---+-----+---+------------+\n",
            "|  1|Test1| 35|      Medior|\n",
            "|  2|Test2| 72|      Senior|\n",
            "|  3|Test3| 16|      Junior|\n",
            "|  4|Test4| 28|      Medior|\n",
            "|  5|Test5| 60|      Senior|\n",
            "+---+-----+---+------------+\n",
            "\n",
            "Processed test data match with expected results!, Test 1 is pass\n",
            "Source and target DataFrame counts match!, Test 2 is pass\n",
            "'age_category' column in target DataFrame has no null values!, Test 3 is pass\n"
          ]
        }
      ],
      "source": [
        "# Import necessary functions\n",
        "!pip install pyspark\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, cast, udf\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
        "\n",
        "# Raw Data ingestion\n",
        "\n",
        "# Create the first source DataFrame with party data\n",
        "pandas_df_src_party = pd.DataFrame({\"id\": [1, 2, 3, 4, 5], \"name\": [\"Alpha\", \"Beta\", \"Charlie\", \"Delta\", \"Echo\"], \"age\": [45, 76, 30, 70, 26]})\n",
        "spark_df_src_data_party = spark.createDataFrame(pandas_df_src_party)\n",
        "spark_df_src_data_party.show()\n",
        "\n",
        "# Create the second source DataFrame with address data\n",
        "pandas_df_src_address = pd.DataFrame({\"id\": [1, 2, 3, 4, 5], \"street\": [\"Barbastraat\", \"Michalstraat\", \"Parijstraat\", \"Tiensesstraat\", \"Dieststraat\"]})\n",
        "spark_df_src_data_address = spark.createDataFrame(pandas_df_src_address)\n",
        "spark_df_src_data_address.show()\n",
        "\n",
        "# Sample ETL stage 1\n",
        "\n",
        "# Join the DataFrames on the \"id\" column\n",
        "joined_df = spark_df_src_data_party.join(spark_df_src_data_address, on=\"id\", how=\"inner\")\n",
        "joined_df.show()\n",
        "\n",
        "# Sample ETL stage 2 (UDF for age category)\n",
        "@udf(returnType=StringType())\n",
        "def get_age_category(age):\n",
        "    if age >= 60:\n",
        "        return \"Senior\"\n",
        "    elif age <= 18:\n",
        "        return \"Junior\"\n",
        "    else:\n",
        "        return \"Medior\"\n",
        "\n",
        "# Add the \"age_category\" column using the UDF\n",
        "df_with_age_category = joined_df.withColumn(\"age_category\", get_age_category(col(\"age\")))\n",
        "\n",
        "# Display the DataFrame with the new column\n",
        "df_with_age_category.show()\n",
        "\n",
        "# Now, let us test the UDF of ETL\n",
        "\n",
        "# Test 1 = Given a fixed set of input data, the real output must match expected output\n",
        "\n",
        "def test_age_category_logic():\n",
        "    \"\"\"\n",
        "    Unit test for the logic of creating the \"age_category\" column based on age.\n",
        "    \"\"\"\n",
        "\n",
        "    # Given INPUT TEST DATA\n",
        "    data = [\n",
        "        (1, \"Test1\", 35),\n",
        "        (2, \"Test2\", 72),\n",
        "        (3, \"Test3\", 16),\n",
        "        (4, \"Test4\", 28),\n",
        "        (5, \"Test5\", 60),\n",
        "    ]\n",
        "    df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
        "\n",
        "    # AND GIVEN Expected results (modify based on your logic)\n",
        "    expected_data = [\n",
        "        (1, \"Test1\", 35, \"Medior\"),\n",
        "        (2, \"Test2\", 72, \"Senior\"),\n",
        "        (3, \"Test3\", 16, \"Junior\"),\n",
        "        (4, \"Test4\", 28, \"Medior\"),\n",
        "        (5, \"Test5\", 60, \"Senior\"),\n",
        "    ]\n",
        "\n",
        "\n",
        "    # WHEN applying the UDF (from above function) to create the \"age_category\" column\n",
        "    df_with_category = df.withColumn(\"age_category\", get_age_category(col(\"age\")))\n",
        "    df_with_category.show()\n",
        "\n",
        "    # THEN Assert the results match expectations\n",
        "    expected_df = spark.createDataFrame(expected_data, [\"id\", \"name\", \"age\", \"age_category\"])\n",
        "    expected_df.show()\n",
        "    assert df_with_category.collect() == expected_df.collect(), \"Processed test data does not match with expected results, Test 1 is failed\"\n",
        "    print(\"Processed test data match with expected results!, Test 1 is pass\")\n",
        "\n",
        "test_age_category_logic()\n",
        "\n",
        "# Test 2 = Checking the source and target count\n",
        "def test_source_target_count_match():\n",
        "\n",
        "  # Get source and target DataFrame row counts\n",
        "  source_df_count = spark_df_src_data_party.count()  # Source data is in df_src_data_party\n",
        "  target_df_count = df_with_age_category.count() # Target data is in df_with_age_category\n",
        "\n",
        "  # Assert that the counts match\n",
        "  assert source_df_count == target_df_count, \"Source and target DataFrame counts do not match, Test 2 is failed\"\n",
        "  print(\"Source and target DataFrame counts match!, Test 2 is pass\")\n",
        "\n",
        "test_source_target_count_match()\n",
        "\n",
        "# Test 3 = Checking data quality in target\n",
        "\n",
        "def test_age_category_not_null():\n",
        "\n",
        "  # Filter rows with null values in \"age_category\"\n",
        "  df_with_nulls = df_with_age_category.filter(col(\"age_category\").isNull())\n",
        "\n",
        "  # Assert that there are no rows with null values\n",
        "  assert df_with_nulls.count() == 0, \"Target DataFrame contains null values in 'age_category' column, Test 3 is failed\"\n",
        "\n",
        "  print(\"'age_category' column in target DataFrame has no null values!, Test 3 is pass\")\n",
        "\n",
        "test_age_category_not_null()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3C/AOvA/YaKZT0el0OVqB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}