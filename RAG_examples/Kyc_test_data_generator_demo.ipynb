{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MAY2704/ML_QEA_usecases/blob/main/Examples/Kyc_test_data_generator_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5YBchGCQUGi",
        "outputId": "6c13d980-1a5d-41b7-fb83-011894f79306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (26.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your desired pattern (e.g., 'Customer {name} with exposure {exposure_type}'): Customer {name} with exposure {exposure_type}\n",
            "Enter the number of data points to generate: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Synthetic Data:\n",
            "{'name': 'Jenny Gonzalez', 'exposure_type': 'Adverse Media Coverage', 'description': 'Customer Jenny Gonzalez with exposure Adverse Media Coveragerestling fren char Multiplayer vi betray Procedure Procedure enable enableMarginalMarginalMarginalinky vi photographers photographers photographers deserves enable enableKindKind Procedure Amit enable bedrooms Procedure Thr fortified Multiplayer enableinkyKindKind 367 367 367inky bedrooms'}\n",
            "{'name': 'John Gomez', 'exposure_type': 'PEP (Politically Exposed Person)', 'description': 'Customer John Gomez with exposure PEP (Politically Exposed Person) deserves deserves deservestoc deserves deservesonoonoono Procedure deserves deservesjudjud Alban Alban Alban persecut deserves deservesMarginal deserves deserves Alban deserves deserves catch persecut deserves persecut deserves Alban persecut persecut persecut'}\n",
            "{'name': 'Mary Gallegos', 'exposure_type': 'PEP (Politically Exposed Person)', 'description': 'Customer Mary Gallegos with exposure PEP (Politically Exposed Person) deserves deserves deserves Scientists deserves deserves Changing deserves deservesonoonoKS deserves deserves Alban Alban Fiji Scientists Procedure AEKSKS Procedure ProcedureKS concoct Procedure AE AEMarginalMarginal Procedure Amit'}\n",
            "{'name': 'Heather Flores', 'exposure_type': 'Adverse Media Coverage', 'description': 'Customer Heather Flores with exposure Adverse Media Coverage deserves deserves deserves curs deserves deserves insurgjud Alban Alban Alban enable Alban Alban persecut persecut persecutMarginal Alban Alban Fiji persecut persecut Alban Alban faux persecut persecut ib persecut Alban persecutMarginal persecut persecut Bitcoin Alban Alban demon persecut'}\n",
            "{'name': 'Heather Garza', 'exposure_type': 'PEP (Politically Exposed Person)', 'description': 'Customer Heather Garza with exposure PEP (Politically Exposed Person) Procedure untreated untreated untreated Tridentlining 367 367 KLquickShipAvailable Edison Alban Alban Alban EuropaMarginalMarginalMarginal KL enable ProcedureMarginal KL KL KL Procedure KL KLlining KL KL Galactic KL KL'}\n",
            "{'name': 'Ryan Hart', 'exposure_type': 'HNWI (High Net Worth Individual)', 'description': 'Customer Ryan Hart with exposure HNWI (High Net Worth Individual) tonnes tonnes KL KL enable Procedure Procedure Europa Europa Procedure Proxy Procedure Europa475475475 relational plague plague deserves deserves deserves photographers deserves deserves scrub scrub photographers photographers Alban humble humble humble radarPrev'}\n",
            "{'name': 'Heather Tapia', 'exposure_type': 'HNWI (High Net Worth Individual)', 'description': 'Customer Heather Tapia with exposure HNWI (High Net Worth Individual) tonnes tonnes KL KL enable pac Alban Alban159 enablelining KL KL KL Saskatchewan KL Procedure KL KL Procedure Thr KL Europa KL Alban Alban Alban buddies EuropaMarginal KL enable Alban Alban'}\n",
            "{'name': 'Sarah Clay', 'exposure_type': 'Adverse Media Coverage', 'description': 'Customer Sarah Clay with exposure Adverse Media Coverage deserves deserves deserves glared glared glared XXX glared Edison Edison Edison tomorrow tomorrow tomorrow relational475475475 relational relational relationalpppp relational relationalilver Scientists Scientists Scientists deserves deservesFlor deserves deserves relational deserves relational Saskatchewan relational475'}\n",
            "{'name': 'Tracy Hendrix', 'exposure_type': 'HNWI (High Net Worth Individual)', 'description': 'Customer Tracy Hendrix with exposure HNWI (High Net Worth Individual) tonnes tonnes KL KL enable statue vi Repl 196 196 Europa Europa Saskatchewan Alban Alban Alban persecut photographers photographers enable enable bedroomschemy plague plague demon demon demon photographers photographers photographers Alban photographers enable'}\n",
            "{'name': 'Amber Romero', 'exposure_type': 'HNWI (High Net Worth Individual)', 'description': 'Customer Amber Romero with exposure HNWI (High Net Worth Individual) Procedure Spears tonnes KL enable ProcedureSeveral deserves glared Procedure glared Edison Edison Edison tomorrow tomorrow Europa Europa Procedure Procedure glared Procedure Amit Amit KL Europa Procedure KL KLemenemen Procedure Procedure Procedure Europa'}\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets faker\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import faker\n",
        "\n",
        "import random\n",
        "\n",
        "# Load pre-trained text-generation model (adjust model name and size)\n",
        "generator = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
        "\n",
        "# Initialize Faker library for random data generation\n",
        "fake = faker.Faker()\n",
        "\n",
        "# User input for test data parameters\n",
        "pattern = input(\"Enter your desired pattern (e.g., 'Customer {name} with exposure {exposure_type}'): \")\n",
        "schema = {\n",
        "  \"name\": \"str\",  # String for customer name\n",
        "  \"exposure_type\": [\"PEP (Politically Exposed Person)\", \"HNWI (High Net Worth Individual)\", \"Sanctioned List\", \"Adverse Media Coverage\"]  # List of possible loan types\n",
        "}\n",
        "\n",
        "# Function to validate user input against schema\n",
        "def validate_pattern(pattern, schema):\n",
        "  for key in schema:\n",
        "    if \"{\" + key + \"}\" not in pattern:\n",
        "      raise ValueError(f\"Missing key '{key}' in the provided pattern.\")\n",
        "\n",
        "validate_pattern(pattern, schema)\n",
        "\n",
        "def generate_data_point(pattern, schema, generator):\n",
        "  data = {}\n",
        "  for key, value_type in schema.items():\n",
        "    if value_type == \"str\":\n",
        "      # Use faker for name generation\n",
        "      data[key] = fake.name()\n",
        "    elif isinstance(value_type, list):\n",
        "      # Choose a random element from the list for categorical data\n",
        "      data[key] = random.choice(value_type)\n",
        "    else:\n",
        "      raise ValueError(f\"Unsupported data type '{value_type}' for key '{key}'.\")\n",
        "\n",
        "  # Use the BART model to generate additional details based on the pattern\n",
        "  filled_pattern = pattern.format(**data)\n",
        "  completion = generator(filled_pattern, max_length=50, num_return_sequences=1)[0][\"generated_text\"]\n",
        "  data[\"description\"] = completion.strip()  # Extract generated details\n",
        "  return data\n",
        "\n",
        "# Generate multiple data points based on user input\n",
        "num_data_points = int(input(\"Enter the number of data points to generate: \"))\n",
        "data = []\n",
        "for _ in range(num_data_points):\n",
        "  data.append(generate_data_point(pattern, schema, generator))\n",
        "\n",
        "print(\"Generated Synthetic Data:\")\n",
        "for point in data:\n",
        "  print(point)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUREvbnoe1V3fUPDWlvf1A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}